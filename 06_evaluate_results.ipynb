{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate results.\n",
    "This notebook creates visualisations of the violations created at previous steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import json\n",
    "import os\n",
    "from utils import get_aws_profile_name, get_aws_iam_role, get_trial_name\n",
    "import generate_altered\n",
    "from generate_altered import TransformsEnum\n",
    "import pandas as pd\n",
    "\n",
    "LOCAL_EXECUTION = True\n",
    "\n",
    "if LOCAL_EXECUTION:\n",
    "    sess = boto3.Session(profile_name=get_aws_profile_name())\n",
    "    sm = sess.client(\"sagemaker\")\n",
    "    iam = sess.client('iam')\n",
    "    role = iam.get_role(RoleName=get_aws_iam_role())['Role']['Arn']\n",
    "else:\n",
    "    sess = boto3.Session()\n",
    "    sm = sess.client(\"sagemaker\")\n",
    "    role = sagemaker.get_execution_role()\n",
    "\n",
    "sagemaker_session = sagemaker.Session(boto_session=sess)\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"model-monitor-bring-your-own-model/\"\n",
    "region = sess.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from botocore.exceptions import ClientError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_trial_component(transform:TransformsEnum):\n",
    "    trial_name = get_trial_name(transform.__name__.replace(\"_\",\"-\"))\n",
    "    component = sm.describe_trial_component(TrialComponentName=trial_name)\n",
    "    return component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_baseline_data_violations(violations):\n",
    "    # Below function may not display properly for multiple metrics with variable thresholds\n",
    "    metrics = [d[\"feature_name\"] for d in violations if d['constraint_check_type']=='baseline_drift_check']\n",
    "    if len(metrics) < 1 : print(\"No feature baseline violation found - exiting plot_baseline_data_violations\"); return\n",
    "    data = [[],[]]\n",
    "    for i, metric in enumerate(metrics):\n",
    "        text = [d['description'] for d in violations if d.get(\"feature_name\")==metric][0]\n",
    "        values = re.findall(r'[\\D\\.]*([\\d\\.]+)', text)\n",
    "        values = [float(v) for v in values]\n",
    "\n",
    "        data[0].append(values[0])\n",
    "        data[1].append(values[1])\n",
    "\n",
    "\n",
    "    X = np.arange(len(metrics))\n",
    "    threshold = data[1]\n",
    "    values = np.array(data[0])\n",
    "    x = range(len(values))\n",
    "\n",
    "    # split it up\n",
    "    above_threshold = np.maximum(values - threshold, 0)\n",
    "    below_threshold = np.minimum(values, threshold)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "    bar_width = .25\n",
    "    ax.bar(x, below_threshold, 0.35, color=\"g\")\n",
    "    ax.bar(x, above_threshold, 0.35, color=\"r\", bottom=below_threshold, label=\"observed exceeding threshold\")\n",
    "    ax.plot([-1., 1.], [0.1, 0.1], \"k--\")\n",
    "\n",
    "    ax.set_xlabel('Metric')\n",
    "    ax.set_ylabel('Metric value')\n",
    "    ax.set_title('Feature baseline drift was detected')\n",
    "    ax.set_xticks(X + bar_width / 2)\n",
    "    ax.set_xticklabels(metrics)\n",
    "    ax.legend(bbox_to_anchor=(1, 1) )\n",
    "    ax.text(-1., threshold[0]*1.02, ' threshold')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model_performance_violations(violations):\n",
    "    metrics = [\"precision\", \"auc\", \"accuracy\"]\n",
    "    data = [[],[]]\n",
    "    for i, metric in enumerate(metrics):\n",
    "        text = [d['description'] for d in violations if d.get(\"metric_name\")==metric][0]\n",
    "        values = re.findall(r'[\\D\\.]*([\\d\\.]+)', text)\n",
    "        values = [float(v) for v in values]\n",
    "\n",
    "        data[0].append(values[0])\n",
    "        data[1].append(values[2])\n",
    "\n",
    "    X = np.arange(len(metrics))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "    bar_width = .25\n",
    "    ax.bar(X , data[0], color = 'b', width = bar_width, label=\"observed\")\n",
    "    ax.bar(X + bar_width, data[1], color = 'g', width = bar_width, label=\"baseline\")\n",
    "\n",
    "    ax.set_xlabel('Metric')\n",
    "    ax.set_ylabel('Metric value')\n",
    "    ax.set_title('Model performance deterioration was detected')\n",
    "    ax.set_xticks(X + bar_width / 2)\n",
    "    ax.set_xticklabels(metrics)\n",
    "    ax.legend(bbox_to_anchor=(1, 1) )\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_label_drift():\n",
    "    original_df = pd.read_csv(\"data/train.csv\")\n",
    "    altered_df = TransformsEnum.LABEL_DRIFT(generate_altered.load_data())\n",
    "    print(original_df.credit_risk.mean())\n",
    "    print(altered_df.credit_risk.mean())\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.style.use('seaborn-darkgrid')\n",
    "    X = np.arange(1)\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "    bar_width = .25\n",
    "    ax.bar(X , altered_df.credit_risk.mean()*100, color = 'b', width = bar_width, label=\"observed\")\n",
    "    ax.bar(X + bar_width, original_df.credit_risk.mean()*100, color = 'g', width = bar_width, label=\"baseline\")\n",
    "\n",
    "    ax.set_xlabel('Metric')\n",
    "    ax.set_ylabel('Metric value')\n",
    "    ax.set_title(f'Percentage of credit accepted')\n",
    "    ax.set_xticks(X + bar_width / 2)\n",
    "    ax.set_xticklabels([\"Percentage of credit accepted\"])\n",
    "    plt.xlim([-.5, 1])\n",
    "    ax.legend(bbox_to_anchor=(1, 1) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bias_violations(violations, focus_metric=None):\n",
    "    # for explanation of metrics see: https://pages.awscloud.com/rs/112-TZM-766/images/Fairness.Measures.for.Machine.Learning.in.Finance.pdf \n",
    "    metrics = [d[\"metric_name\"] for d in violations if d['constraint_check_type']=='bias_drift_check']\n",
    "    data = [[],[]]\n",
    "    for i, metric in enumerate(metrics):\n",
    "        text = [d['description'] for d in violations if d.get(\"metric_name\")==metric][0]\n",
    "        values = re.findall(r'[\\D\\.]*([\\d\\.]+)', text)\n",
    "        values = [float(v) for v in values]\n",
    "\n",
    "        data[0].append(values[0])\n",
    "        data[1].append(values[1])\n",
    "\n",
    "    X = np.arange(len(metrics))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "    bar_width = .25\n",
    "    ax.bar(X , data[0], color = 'b', width = bar_width, label=\"observed\")\n",
    "    ax.bar(X + bar_width, data[1], color = 'g', width = bar_width, label=\"baseline\")\n",
    "\n",
    "    ax.set_xlabel('Metric')\n",
    "    ax.set_ylabel('Metric value')\n",
    "    ax.set_title('Bias drift check triggered for the following')\n",
    "    ax.set_xticks(X + bar_width / 2)\n",
    "    ax.set_xticklabels(metrics)\n",
    "    ax.legend(bbox_to_anchor=(1, 1) )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    if focus_metric:\n",
    "        loc = metrics.index(focus_metric)\n",
    "        fig = plt.figure()\n",
    "        plt.style.use('seaborn-darkgrid')\n",
    "        X = np.arange(1)\n",
    "        ax = fig.add_axes([0,0,1,1])\n",
    "        bar_width = .25\n",
    "        ax.bar(X , data[0][loc], color = 'b', width = bar_width, label=\"observed\")\n",
    "        ax.bar(X + bar_width, data[1][loc], color = 'g', width = bar_width, label=\"baseline\")\n",
    "\n",
    "        ax.set_xlabel('Metric')\n",
    "        ax.set_ylabel('Metric value')\n",
    "        ax.set_title(f'Bias drift check triggered for the metric {focus_metric}')\n",
    "        ax.set_xticks(X + bar_width / 2)\n",
    "        ax.set_xticklabels([focus_metric])\n",
    "        plt.xlim([-.5, 1])\n",
    "        ax.legend(bbox_to_anchor=(1, 1) )\n",
    "\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "\n",
    "def plot_model_explainability_violations(violations, job_output_uri):\n",
    "    \n",
    "    metrics = [\"shap\"]\n",
    "    data = [[],[]]\n",
    "    features = []\n",
    "    for i, metric in enumerate(metrics):\n",
    "        text = [d['description'] for d in violations if d.get(\"metric_name\")==metric][0]\n",
    "        values = re.findall(r'[\\D\\.]*([\\d\\.]+)', text)\n",
    "        values = [float(v) for v in values]\n",
    "\n",
    "        ndcg = values[0]\n",
    "        baseline_ndcg = values[1]\n",
    "\n",
    "    print(f\"NDCG (Normalized Discounted Cumulative Gain) computed was {ndcg}\")\n",
    "\n",
    "    analysis_uri = job_output_uri.replace(\"constraint_violations\", \"analysis\")\n",
    "    analysis_json = json.loads(sagemaker.s3.S3Downloader().read_file(analysis_uri, sagemaker_session=sagemaker_session))[\"explanations\"][\"kernel_shap\"][\"label0\"][\"global_shap_values\"]\n",
    "\n",
    "    baseline_analysis_uri = utils.get_baseline_uri(\"model-explainability-analysis\")\n",
    "    baseline_analysis_json = json.loads(sagemaker.s3.S3Downloader().read_file(baseline_analysis_uri, sagemaker_session=sagemaker_session))[\"explanations\"][\"kernel_shap\"][\"label0\"][\"global_shap_values\"]\n",
    "\n",
    "\n",
    "    for key in analysis_json.keys():\n",
    "        data[0].append(analysis_json.get(key))\n",
    "        data[1].append(baseline_analysis_json.get(key))\n",
    "        features.append(key)\n",
    "\n",
    "    X = np.arange(len(features))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "    bar_width = .25\n",
    "    ax.barh(X , data[0], color = 'b', height = bar_width, label=\"observed\")\n",
    "    ax.barh(X + bar_width, data[1], color = 'g', height = bar_width, label=\"baseline\")\n",
    "\n",
    "    ax.set_ylabel('Features')\n",
    "    ax.set_xlabel('Feature importance')\n",
    "    ax.set_title('Feature importance differences in observed and baseline datasets')\n",
    "    ax.set_yticks(X + bar_width / 2)\n",
    "    ax.set_yticklabels(features)\n",
    "    ax.legend(bbox_to_anchor=(1, 1) )\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    X = np.arange(len(features[:5]))\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.style.use('seaborn-darkgrid')\n",
    "\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "    bar_width = .25\n",
    "    ax.barh(X , data[0][:5], color = 'b', height = bar_width, label=\"observed\")\n",
    "    ax.barh(X + bar_width, data[1][:5], color = 'g', height = bar_width, label=\"baseline\")\n",
    "\n",
    "    ax.set_ylabel('Features')\n",
    "    ax.set_xlabel('Feature importance')\n",
    "    ax.set_title('Feature importance differences in observed and baseline datasets')\n",
    "    ax.set_yticks(X + bar_width / 2)\n",
    "    ax.set_yticklabels(features[:5])\n",
    "    ax.legend(bbox_to_anchor=(1, 1) )\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_drift(transform=TransformsEnum.FEATURE_DRIFT):\n",
    "    original_df = pd.read_csv(\"data/train.csv\")\n",
    "    altered_df = transform(generate_altered.load_data())\n",
    "    print(original_df.credit_risk.mean())\n",
    "    print(altered_df.credit_risk.mean())\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.style.use('seaborn-darkgrid')\n",
    "    # X = np.arange(3)\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "\n",
    "    ax.hist(original_df[\"duration\"], alpha=0.5, label=\"original\")\n",
    "    ax.hist(altered_df[\"duration\"], alpha=0.5, label=\"observed\")\n",
    "\n",
    "    ax.set_xlabel('Feature: Duration')\n",
    "    ax.set_ylabel('frequency')\n",
    "    ax.set_title(f'Distribution of duration')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Violation link acquired from experiment tracking tool and copy&pasted for convinience \n",
    "component = get_latest_trial_component(TransformsEnum.CONCEPT_DRIFT)\n",
    "\n",
    "violations_uris = [\n",
    "    component['Parameters']['data-quality-violoations']['StringValue'],\n",
    "    component['Parameters']['model-quality-violoations']['StringValue'],\n",
    "    component['Parameters']['model-bias-violoations']['StringValue'],\n",
    "    component['Parameters']['model-explainability-violoations']['StringValue']\n",
    "    ]\n",
    "violations_uris = [v[::-1].replace(\"//\",\"/\",1)[::-1] for v in violations_uris] # Delete when I rerun experiments and don't commit\n",
    "\n",
    "violations = []\n",
    "for violations_uri in violations_uris:\n",
    "    try:\n",
    "        viol = json.loads(sagemaker.s3.S3Downloader().read_file(violations_uri, sagemaker_session=sagemaker_session))\n",
    "\n",
    "        violations += viol[\"violations\"]\n",
    "    except ClientError as ex:\n",
    "        if ex.response['Error']['Code'] == 'NoSuchKey':\n",
    "            print(f\"No violation file {violations_uri} found \")\n",
    "\n",
    "plot_model_performance_violations(violations)\n",
    "plot_bias_violations(violations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this scenario we can see that due to Concept Drift, the model performance of the model was impacted and our precision, along with other metrics had a considerable impact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component = get_latest_trial_component(TransformsEnum.LABEL_DRIFT)\n",
    "\n",
    "violations_uris = [\n",
    "    component['Parameters']['data-quality-violoations']['StringValue'],\n",
    "    component['Parameters']['model-quality-violoations']['StringValue'],\n",
    "    component['Parameters']['model-bias-violoations']['StringValue'],\n",
    "    component['Parameters']['model-explainability-violoations']['StringValue']\n",
    "    ]\n",
    "# violations_uris = [v[::-1].replace(\"//\",\"/\",1)[::-1] for v in violations_uris] # Delete when I rerun experiments and don't commit\n",
    "\n",
    "violations = []\n",
    "for violations_uri in violations_uris:\n",
    "    try:\n",
    "        viol = json.loads(sagemaker.s3.S3Downloader().read_file(violations_uri, sagemaker_session=sagemaker_session))\n",
    "\n",
    "        violations += viol[\"violations\"]\n",
    "    except ClientError as ex:\n",
    "        if ex.response['Error']['Code'] == 'NoSuchKey':\n",
    "            print(f\"No violation file {violations_uri} found \")\n",
    "\n",
    "plot_model_performance_violations(violations)\n",
    "plot_label_drift()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component = get_latest_trial_component(TransformsEnum.BIAS_DRIFT)\n",
    "\n",
    "violations_uris = [\n",
    "    component['Parameters']['data-quality-violoations']['StringValue'],\n",
    "    component['Parameters']['model-quality-violoations']['StringValue'],\n",
    "    component['Parameters']['model-bias-violoations']['StringValue'],\n",
    "    component['Parameters']['model-explainability-violoations']['StringValue']\n",
    "    ]\n",
    "\n",
    "# violations_uris = [v[::-1].replace(\"//\",\"/\",1)[::-1] for v in violations_uris] # Delete when I rerun experiments and don't commit\n",
    "violations = []\n",
    "for violations_uri in violations_uris:\n",
    "    try:\n",
    "        viol = json.loads(sagemaker.s3.S3Downloader().read_file(violations_uri, sagemaker_session=sagemaker_session))\n",
    "\n",
    "        violations += viol[\"violations\"]\n",
    "    except ClientError as ex:\n",
    "        if ex.response['Error']['Code'] == 'NoSuchKey':\n",
    "            print(f\"No violation file {violations_uri} found \")\n",
    "\n",
    "plot_bias_violations(violations, focus_metric=\"DPPL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bias_dppl_expanded():\n",
    "    original_df = pd.read_csv(\"data/train.csv\")\n",
    "    altered_df = TransformsEnum.BIAS_DRIFT(generate_altered.load_data())\n",
    "    print(original_df.credit_risk.mean())\n",
    "    print(altered_df.credit_risk.mean())\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.style.use('seaborn-darkgrid')\n",
    "    X = np.arange(3)\n",
    "    ax = fig.add_axes([0,0,2,1])\n",
    "    bar_width = .25\n",
    "    data_observed = [\n",
    "        altered_df.loc[altered_df.foreign_worker == 1].credit_risk.mean()*100, \n",
    "        altered_df.loc[altered_df.foreign_worker != 1].credit_risk.mean()*100, \n",
    "        altered_df.credit_risk.mean()*100\n",
    "        ]\n",
    "    data_baseline = [\n",
    "        original_df.loc[original_df.foreign_worker == 1].credit_risk.mean()*100,\n",
    "        original_df.loc[original_df.foreign_worker != 1].credit_risk.mean()*100,\n",
    "        original_df.credit_risk.mean()*100\n",
    "        ]\n",
    "    ax.bar(X , data_observed , color = 'b', width = bar_width, label=\"observed\")\n",
    "    ax.bar(X + bar_width, data_baseline, color = 'g', width = bar_width, label=\"baseline\")\n",
    "\n",
    "    ax.set_xlabel('Metric')\n",
    "    ax.set_ylabel('Metric value')\n",
    "    ax.set_title(f'Percentage of credit accepted')\n",
    "    ax.set_xticks(X + bar_width / 2)\n",
    "    ax.set_xticklabels([\"Percentage of Accepted Foreign Workers\", \"Percentage of Accepted non-Foreign Workers\", \"Percentage of All Accepted Workers\"])\n",
    "    ax.legend(bbox_to_anchor=(1, 1) )\n",
    "\n",
    "\n",
    "plot_bias_dppl_expanded()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component = get_latest_trial_component(TransformsEnum.FEATURE_DRIFT)\n",
    "\n",
    "violations_uris = [\n",
    "    component['Parameters']['data-quality-violoations']['StringValue'],\n",
    "    component['Parameters']['model-quality-violoations']['StringValue'],\n",
    "    component['Parameters']['model-bias-violoations']['StringValue'],\n",
    "    component['Parameters']['model-explainability-violoations']['StringValue']\n",
    "    ]\n",
    "\n",
    "# violations_uris = [v[::-1].replace(\"//\",\"/\",1)[::-1] for v in violations_uris] # Delete when I rerun experiments and don't commit\n",
    "violations = []\n",
    "for violations_uri in violations_uris:\n",
    "    try:\n",
    "        viol = json.loads(sagemaker.s3.S3Downloader().read_file(violations_uri, sagemaker_session=sagemaker_session))\n",
    "\n",
    "        violations += viol[\"violations\"]\n",
    "    except ClientError as ex:\n",
    "        if ex.response['Error']['Code'] == 'NoSuchKey':\n",
    "            print(f\"No violation file {violations_uri} found \")\n",
    "\n",
    "plot_baseline_data_violations(violations)\n",
    "plot_feature_drift()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Drift - Systematic issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component = get_latest_trial_component(TransformsEnum.FEATURE_DRIFT_SYSTEMATIC)\n",
    "\n",
    "violations_uris = [\n",
    "    component['Parameters']['data-quality-violoations']['StringValue'],\n",
    "    component['Parameters']['model-quality-violoations']['StringValue'],\n",
    "    component['Parameters']['model-bias-violoations']['StringValue'],\n",
    "    component['Parameters']['model-explainability-violoations']['StringValue']\n",
    "    ]\n",
    "\n",
    "violations = []\n",
    "for violations_uri in violations_uris:\n",
    "    try:\n",
    "        viol = json.loads(sagemaker.s3.S3Downloader().read_file(violations_uri, sagemaker_session=sagemaker_session))\n",
    "\n",
    "        violations += viol[\"violations\"]\n",
    "    except ClientError as ex:\n",
    "        if ex.response['Error']['Code'] == 'NoSuchKey':\n",
    "            print(f\"No violation file {violations_uri} found \")\n",
    "\n",
    "plot_baseline_data_violations(violations)\n",
    "plot_feature_drift(TransformsEnum.FEATURE_DRIFT_SYSTEMATIC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explainability Drift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component = get_latest_trial_component(TransformsEnum.EXPLAINABILITY_DRIFT)\n",
    "\n",
    "violations_uris = [\n",
    "    component['Parameters']['data-quality-violoations']['StringValue'],\n",
    "    component['Parameters']['model-quality-violoations']['StringValue'],\n",
    "    component['Parameters']['model-bias-violoations']['StringValue'],\n",
    "    component['Parameters']['model-explainability-violoations']['StringValue']\n",
    "    ]\n",
    "\n",
    "# violations_uris = [v[::-1].replace(\"//\",\"/\",1)[::-1] for v in violations_uris] # Delete when I rerun experiments and don't commit\n",
    "violations = []\n",
    "for violations_uri in violations_uris:\n",
    "    try:\n",
    "        viol = json.loads(sagemaker.s3.S3Downloader().read_file(violations_uri, sagemaker_session=sagemaker_session))\n",
    "\n",
    "        violations += viol[\"violations\"]\n",
    "    except ClientError as ex:\n",
    "        if ex.response['Error']['Code'] == 'NoSuchKey':\n",
    "            print(f\"No violation file {violations_uri} found \")\n",
    "\n",
    "\n",
    "plot_model_explainability_violations(violations, component['Parameters']['model-explainability-violoations']['StringValue'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('mm-byom')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "66d769f68ed4c84a07e23de83cd77438094586f02a1789a3dfa9b143f2e1d947"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
